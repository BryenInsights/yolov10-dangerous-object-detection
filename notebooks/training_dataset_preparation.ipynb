{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uW4odpLNruXT"
   },
   "source": [
    "# Deep Learning project : Dangerous Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Before training a YOLO model, the dataset must be organized in the right format and optionally enriched with data augmentations.  \n",
    "This notebook guides you through the process of preparing your annotated images and labels for training.  \n",
    "\n",
    "You will:  \n",
    "- Arrange your dataset into the YOLO folder structure (`images/train`, `labels/train`, etc.).  \n",
    "- Apply data augmentations (e.g., flips, brightness/contrast changes) to improve model robustness.  \n",
    "- Validate that images and annotations are properly aligned and ready for training.  \n",
    "\n",
    "By the end, you will have a clean and well-structured dataset that can be directly used in the fine-tuning stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1tFs9a0KrziF"
   },
   "source": [
    "### Downloading Out of the box YOLOv10 Models weights\n",
    "In this section, we download pre-trained YOLOv10 model weights.  \n",
    "These weights allow us to test predictions with the base model before fine-tuning,  \n",
    "so we can compare performance improvements later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 314,
     "status": "ok",
     "timestamp": 1727459577769,
     "user": {
      "displayName": "Bryen Param",
      "userId": "02690284411531247689"
     },
     "user_tz": -120
    },
    "id": "TxF0O-S_rm85",
    "outputId": "709768ca-1771-4240-edc1-586644de16ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.12.6\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 2724,
     "status": "ok",
     "timestamp": 1727459581497,
     "user": {
      "displayName": "Bryen Param",
      "userId": "02690284411531247689"
     },
     "user_tz": -120
    },
    "id": "zbjk0Su2sBLJ",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "8cb25b7c-f5d9-4b5b-f12f-01051436fe24",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'yolov10' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/THU-MIG/yolov10.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 310,
     "status": "ok",
     "timestamp": 1727459600012,
     "user": {
      "displayName": "Bryen Param",
      "userId": "02690284411531247689"
     },
     "user_tz": -120
    },
    "id": "5yEHYxopsM0f",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "95059e42-669f-48bb-fa77-1b13c89e8356",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kowsi/Documents/A22 DSTI CLASS/DeepLearning/object_detection_dl_project/object_detect_dl/yolov10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kowsi/Library/Caches/pypoetry/virtualenvs/object-detect-dl-MR8rY4Ff-py3.12/lib/python3.12/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd yolov10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 9450,
     "status": "ok",
     "timestamp": 1727459610473,
     "user": {
      "displayName": "Bryen Param",
      "userId": "02690284411531247689"
     },
     "user_tz": -120
    },
    "id": "Jk0g0JRJsaEI",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "9b035e4f-f00a-4fd4-f44c-e38635b46e7e"
   },
   "outputs": [],
   "source": [
    "!pip install ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Download weights (e.g., `yolov10n.pt`) and place them in a `weights/` folder. Then update any `weights=`/`model=` arguments to point to your file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 8273,
     "status": "ok",
     "timestamp": 1727460247715,
     "user": {
      "displayName": "Bryen Param",
      "userId": "02690284411531247689"
     },
     "user_tz": -120
    },
    "id": "F8XSSyF7sbCr",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "588141aa-54a9-4e97-fc48-ed8a11901e7b"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "# Create a directory for the weights in the current working directory\n",
    "weights_dir = os.path.join(os.getcwd(), \"weights\")\n",
    "os.makedirs(weights_dir, exist_ok=True)\n",
    "\n",
    "# URLs of the weight files\n",
    "urls = [\n",
    "    \"https://github.com/jameslahm/yolov10/releases/download/v1.0/yolov10n.pt\",\n",
    "    \"https://github.com/jameslahm/yolov10/releases/download/v1.0/yolov10s.pt\",\n",
    "    \"https://github.com/jameslahm/yolov10/releases/download/v1.0/yolov10m.pt\",\n",
    "    \"https://github.com/jameslahm/yolov10/releases/download/v1.0/yolov10b.pt\",\n",
    "    \"https://github.com/jameslahm/yolov10/releases/download/v1.0/yolov10x.pt\",\n",
    "    \"https://github.com/jameslahm/yolov10/releases/download/v1.0/yolov10l.pt\"\n",
    "]\n",
    "\n",
    "# Download each file\n",
    "for url in urls:\n",
    "  file_name = os.path.join(weights_dir, os.path.basename(url))\n",
    "  urllib. request.urlretrieve(url, file_name)\n",
    "  print(f\"Downloaded {file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test prediction with base YOLOv10n Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 11094,
     "status": "ok",
     "timestamp": 1726834526283,
     "user": {
      "displayName": "Bryen Param",
      "userId": "02690284411531247689"
     },
     "user_tz": -120
    },
    "id": "f06PVLofvMMI",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "216b7c81-a189-4e29-f8a3-7ba849e208ad",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ultralytics/nn/tasks.py:733: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(file, map_location=\"cpu\")\n",
      "Ultralytics YOLOv8.1.34 ðŸš€ Python-3.10.12 torch-2.4.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
      "YOLOv10n summary (fused): 285 layers, 2762608 parameters, 63840 gradients, 8.6 GFLOPs\n",
      "\n",
      "image 1/1 /content/yolov10/test_images/IMG_8455.jpg: 480x640 1 0, 1 63, 1 76, 41.3ms\n",
      "Speed: 14.2ms preprocess, 41.3ms inference, 396.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
      "ðŸ’¡ Learn more at https://docs.ultralytics.com/modes/predict\n"
     ]
    }
   ],
   "source": [
    "# Update the values below to point to YOUR files:\n",
    "#  - source=<path to your image/video/folder>\n",
    "#  - weights/model=<path to your .pt weights> (e.g., ../weights/yolov10n.pt)\n",
    "!yolo task=detect mode=predict conf=0.25 save=True model=../weights/yolov10n.pt source=test.mp4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5lmRv3NBpn1a"
   },
   "source": [
    "### Data Augmentation\n",
    "Here we apply data augmentation techniques (such as flips, brightness/contrast changes, or noise).  \n",
    "The goal is to increase dataset variety, improve robustness, and help the model generalize better  \n",
    "to different real-world conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 3209,
     "status": "ok",
     "timestamp": 1727453935189,
     "user": {
      "displayName": "Bryen Param",
      "userId": "02690284411531247689"
     },
     "user_tz": -120
    },
    "id": "s_OkivFtOCpj",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "ef9831e2-d562-4713-b5a8-4e7bafe6202d"
   },
   "outputs": [],
   "source": [
    "pip install albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2603,
     "status": "ok",
     "timestamp": 1727456752661,
     "user": {
      "displayName": "Bryen Param",
      "userId": "02690284411531247689"
     },
     "user_tz": -120
    },
    "id": "sRdZLvlWch_0",
    "outputId": "b2c887c1-cc04-4cf1-e118-734b57ead3d5"
   },
   "outputs": [],
   "source": [
    "# Install tqdm to display progress bars while processing images.\n",
    "pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starter cell for a **simple data augmentation** pipeline (resize, flips, color jitter, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First simple Data augmentation\n",
    "augmentations = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),  # Flip the image horizontally 50% of the time\n",
    "    A.VerticalFlip(p=0.5),    # Flip the image vertically 50% of the time\n",
    "    A.RandomBrightnessContrast(p=1),  # Always adjust brightness and contrast\n",
    "    A.MotionBlur(p=1),        # Always apply motion blur\n",
    "    A.HueSaturationValue(p=1),  # Always adjust hue, saturation, and value\n",
    "], bbox_params=A.BboxParams(format='yolo', label_fields=['category_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Template for a **richer augmentation pipeline** (e.g., geometric transforms, cutout, blur)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More Complex Data augmentation\n",
    "augmentations_2 = A.Compose([\n",
    "    # Rotation for object angles\n",
    "    A.Rotate(limit=45, p=1),\n",
    "    # Perspective changes\n",
    "    A.Affine(scale=(0.8, 1.2), translate_percent=(0.1, 0.1), rotate=0, shear=15, p=0.5),\n",
    "    # Light adjustments\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=1),\n",
    "    # Light effects\n",
    "    A.RandomShadow(shadow_roi=(0, 0.5, 1, 1), shadow_dimension=5, p=0.5),\n",
    "    A.RandomRain(slant_lower=-10, slant_upper=10, p=0.2),  # Light reflections\n",
    "    # Motion effects\n",
    "    A.MotionBlur(blur_limit=(3, 7), p=0.5), \n",
    "    # Image noise\n",
    "    A.GaussNoise(var_limit=(10.0, 50.0), p=0.5),\n",
    "    # Color changes\n",
    "    A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=1),\n",
    "    # Simulate blocked views\n",
    "    A.CoarseDropout(max_holes=8, max_height=16, max_width=16, min_holes=1, min_height=8, min_width=8, fill_value=0, p=0.5),\n",
    "], bbox_params=A.BboxParams(format='yolo', label_fields=['category_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "r4W7NVZOenCT",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_yolo_annotation(annotation_file):\n",
    "    \"\"\"Load YOLO format annotations from a file.\"\"\"\n",
    "    with open(annotation_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    # Extract bounding boxes and category IDs\n",
    "    bboxes = [list(map(float, line.strip().split()[1:])) for line in lines]\n",
    "    category_ids = [int(line.strip().split()[0]) for line in lines]\n",
    "    return bboxes, category_ids\n",
    "\n",
    "def save_augmented_data(image, bboxes, category_ids, output_img_path, output_label_path):\n",
    "    \"\"\"Save the augmented image and its corresponding YOLO labels.\"\"\"\n",
    "    cv2.imwrite(output_img_path, image)\n",
    "    with open(output_label_path, 'w') as f:\n",
    "        for bbox, class_id in zip(bboxes, category_ids):\n",
    "            f.write(f\"{class_id} {' '.join(map(str, bbox))}\\n\")\n",
    "\n",
    "def augment_data(image_dir, label_dir, output_image_dir, output_label_dir):\n",
    "    \"\"\"Main function to augment data.\"\"\"\n",
    "    # Create output directories if they don't exist\n",
    "    os.makedirs(output_image_dir, exist_ok=True)\n",
    "    os.makedirs(output_label_dir, exist_ok=True)\n",
    "\n",
    "    # Get all image files\n",
    "    image_files = [f for f in os.listdir(image_dir) if f.lower().endswith((\".jpeg\", \".jpg\"))]\n",
    "\n",
    "    # Process each image\n",
    "    for img_file in tqdm(image_files, desc=\"Augmenting Images\"):\n",
    "        img_path = os.path.join(image_dir, img_file)\n",
    "        label_path = os.path.join(label_dir, os.path.splitext(img_file)[0] + \".txt\")\n",
    "\n",
    "        # Read the image and its annotations\n",
    "        image = cv2.imread(img_path)\n",
    "        bboxes, category_ids = load_yolo_annotation(label_path)\n",
    "\n",
    "        # Save the original image and labels\n",
    "        save_augmented_data(image, bboxes, category_ids,\n",
    "                            os.path.join(output_image_dir, img_file),\n",
    "                            os.path.join(output_label_dir, os.path.splitext(img_file)[0] + \".txt\"))\n",
    "\n",
    "        # Generate 10 augmented versions of each image \n",
    "        for i in range(10): # Change the number of versions if needed\n",
    "            # Apply augmentations -> Select the appropriate augmentation\n",
    "            augmented = augmentations(image=image, bboxes=bboxes, category_ids=category_ids)\n",
    "\n",
    "            # Create unique filenames for augmented data\n",
    "            output_img_path = os.path.join(output_image_dir, f\"{os.path.splitext(img_file)[0]}_aug_{i+1}.jpg\")\n",
    "            output_label_path = os.path.join(output_label_dir, f\"{os.path.splitext(img_file)[0]}_aug_{i+1}.txt\")\n",
    "\n",
    "            # Save augmented image and updated labels\n",
    "            save_augmented_data(augmented['image'], augmented['bboxes'], augmented['category_ids'],\n",
    "                                output_img_path, output_label_path)\n",
    "\n",
    "    # Calculate and print total number of images generated\n",
    "    total_images = len(image_files) * 11  # Original + 10 augmented versions\n",
    "    print(f\"Total images generated: {total_images}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set these paths to match your file structure\n",
    "    image_dir = \"original_data/first_approach_multi_env_data/multi_env_original_images\"\n",
    "    label_dir = \"original_data/first_approach_multi_env_data/multi_env_original_labels\"\n",
    "    output_image_dir = \"final_augmented_base_images\"\n",
    "    output_label_dir = \"final_augmented_base_labels\"\n",
    "\n",
    "    # Run the augmentation process\n",
    "    augment_data(image_dir, label_dir, output_image_dir, output_label_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tTO9DRUhpeMG"
   },
   "source": [
    "## Rearange dataset structure for fine tuning\n",
    "In this step, we organize the dataset into the YOLO format (`images/train`, `images/val`, `labels/train`, `labels/val`).  \n",
    "This structure is required for training and ensures that YOLO can properly read both images and annotations  \n",
    "during the fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data pairs found: 252\n",
      "Dataset has been split and copied successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Set seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Define paths for images and labels\n",
    "images_path = 'final_augmented_base_images'\n",
    "labels_path = 'final_augmented_base_labels'\n",
    "\n",
    "# Destination directories for images and labels\n",
    "train_dir = 'datasets/images/train'\n",
    "val_dir = 'datasets/images/val'\n",
    "test_dir = 'datasets/images/test'\n",
    "train_label_dir = 'datasets/labels/train'\n",
    "val_label_dir = 'datasets/labels/val'\n",
    "test_label_dir = 'datasets/labels/test'\n",
    "\n",
    "# Create the directories if they don't exist\n",
    "Path(train_dir).mkdir(parents=True, exist_ok=True)\n",
    "Path(val_dir).mkdir(parents=True, exist_ok=True)\n",
    "Path(test_dir).mkdir(parents=True, exist_ok=True)\n",
    "Path(train_label_dir).mkdir(parents=True, exist_ok=True)\n",
    "Path(val_label_dir).mkdir(parents=True, exist_ok=True)\n",
    "Path(test_label_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# List all images (allowing for different cases in extensions)\n",
    "images = [f for f in os.listdir(images_path) if f.lower().endswith(('.jpg', '.jpeg'))]\n",
    "\n",
    "# List all labels\n",
    "labels = [f for f in os.listdir(labels_path) if f.endswith('.txt')]\n",
    "\n",
    "# Ensure each image has a corresponding label\n",
    "# We match by base name, ignoring file extension case (i.e., 'image1.jpg' with 'image1.txt')\n",
    "images = sorted(images)\n",
    "labels = sorted(labels)\n",
    "# Filter the data to only include pairs where both image and label exist\n",
    "data = []\n",
    "for image_file in images:\n",
    "    image_base = os.path.splitext(image_file)[0]\n",
    "    label_file = f\"{image_base}.txt\"\n",
    "    if label_file in labels:\n",
    "        data.append((image_file, label_file))\n",
    "# Check that we have valid image-label pairs\n",
    "assert len(data) > 0, \"No matching image-label pairs found!\"\n",
    "print(f\"Total data pairs found: {len(data)}\")\n",
    "\n",
    "# Shuffle the data\n",
    "random.shuffle(data)\n",
    "\n",
    "# Split the dataset (80% train, 10% val, 10% test)\n",
    "train_split = int(0.8 * len(data))\n",
    "val_split = int(0.9 * len(data))\n",
    "train_data = data[:train_split]\n",
    "val_data = data[train_split:val_split]\n",
    "test_data = data[val_split:]\n",
    "\n",
    "# Function to copy files\n",
    "def copy_files(data, image_dest, label_dest):\n",
    "    for image_file, label_file in data:\n",
    "        shutil.copy(os.path.join(images_path, image_file), os.path.join(image_dest, image_file))\n",
    "        shutil.copy(os.path.join(labels_path, label_file), os.path.join(label_dest, label_file))\n",
    "        \n",
    "# Copy train, val, and test files\n",
    "copy_files(train_data, train_dir, train_label_dir)\n",
    "copy_files(val_data, val_dir, val_label_dir)\n",
    "copy_files(test_data, test_dir, test_label_dir)\n",
    "print(\"Dataset has been split and copied successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook prepared the dataset for fine-tuning our YOLOv10 model.  \n",
    "We organized the data into the correct YOLO format, applied augmentations to improve robustness,  \n",
    "and verified that annotations and images are aligned.  \n",
    "\n",
    "With the dataset now ready, the next step is to use it in the training pipeline (`finetuning_yolo.ipynb`)  \n",
    "to fine-tune the model and evaluate its performance on our custom task."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyORI5J9mDOP7OMWhYRWPPRi",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
