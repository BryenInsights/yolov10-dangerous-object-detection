{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uW4odpLNruXT"
   },
   "source": [
    "# Deep Learning project : Dangerous Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Before training a YOLO model, the dataset must be organized in the right format and optionally enriched with data augmentations.  \n",
    "This notebook guides you through the process of preparing your annotated images and labels for training.  \n",
    "\n",
    "You will:  \n",
    "- Arrange your dataset into the YOLO folder structure (`images/train`, `labels/train`, etc.).  \n",
    "- Apply data augmentations (e.g., flips, brightness/contrast changes) to improve model robustness.  \n",
    "- Validate that images and annotations are properly aligned and ready for training.  \n",
    "\n",
    "By the end, you will have a clean and well-structured dataset that can be directly used in the fine-tuning stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1tFs9a0KrziF"
   },
   "source": [
    "### Downloading Out of the box YOLOv10 Models weights\n",
    "In this section, we download pre-trained YOLOv10 model weights.  \n",
    "These weights allow us to test predictions with the base model before fine-tuning,  \n",
    "so we can compare performance improvements later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 314,
     "status": "ok",
     "timestamp": 1727459577769,
     "user": {
      "displayName": "Bryen Param",
      "userId": "02690284411531247689"
     },
     "user_tz": -120
    },
    "id": "TxF0O-S_rm85",
    "outputId": "709768ca-1771-4240-edc1-586644de16ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.12.6\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 2724,
     "status": "ok",
     "timestamp": 1727459581497,
     "user": {
      "displayName": "Bryen Param",
      "userId": "02690284411531247689"
     },
     "user_tz": -120
    },
    "id": "zbjk0Su2sBLJ",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "8cb25b7c-f5d9-4b5b-f12f-01051436fe24",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'yolov10' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/THU-MIG/yolov10.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 310,
     "status": "ok",
     "timestamp": 1727459600012,
     "user": {
      "displayName": "Bryen Param",
      "userId": "02690284411531247689"
     },
     "user_tz": -120
    },
    "id": "5yEHYxopsM0f",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "95059e42-669f-48bb-fa77-1b13c89e8356",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kowsi/Documents/A22 DSTI CLASS/DeepLearning/object_detection_dl_project/object_detect_dl/yolov10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kowsi/Library/Caches/pypoetry/virtualenvs/object-detect-dl-MR8rY4Ff-py3.12/lib/python3.12/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "cd yolov10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 9450,
     "status": "ok",
     "timestamp": 1727459610473,
     "user": {
      "displayName": "Bryen Param",
      "userId": "02690284411531247689"
     },
     "user_tz": -120
    },
    "id": "Jk0g0JRJsaEI",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "9b035e4f-f00a-4fd4-f44c-e38635b46e7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /Users/kowsi/Documents/A22 DSTI CLASS/DeepLearning/object_detection_dl_project/object_detect_dl/yolov10\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: matplotlib>=3.3.0 in /Users/kowsi/Library/Caches/pypoetry/virtualenvs/object-detect-dl-MR8rY4Ff-py3.12/lib/python3.12/site-packages (from ultralytics==8.1.34) (3.9.2)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in /Users/kowsi/Library/Caches/pypoetry/virtualenvs/object-detect-dl-MR8rY4Ff-py3.12/lib/python3.12/site-packages (from ultralytics==8.1.34) (4.10.0.84)\n",
      "Requirement already satisfied: pillow>=7.1.2 in /Users/kowsi/Library/Caches/pypoetry/virtualenvs/object-detect-dl-MR8rY4Ff-py3.12/lib/python3.12/site-packages (from ultralytics==8.1.34) (10.4.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /Users/kowsi/Library/Caches/pypoetry/virtualenvs/object-detect-dl-MR8rY4Ff-py3.12/lib/python3.12/site-packages (from ultralytics==8.1.34) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.23.0 in /Users/kowsi/Library/Caches/pypoetry/virtualenvs/object-detect-dl-MR8rY4Ff-py3.12/lib/python3.12/site-packages (from ultralytics==8.1.34) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /Users/kowsi/Library/Caches/pypoetry/virtualenvs/object-detect-dl-MR8rY4Ff-py3.12/lib/python3.12/site-packages (from ultralytics==8.1.34) (1.14.1)\n",
      "Requirement already satisfied: torch>=1.8.0 in /Users/kowsi/Library/Caches/pypoetry/virtualenvs/object-detect-dl-MR8rY4Ff-py3.12/lib/python3.12/site-packages (from ultralytics==8.1.34) (2.4.1)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in /Users/kowsi/Library/Caches/pypoetry/virtualenvs/object-detect-dl-MR8rY4Ff-py3.12/lib/python3.12/site-packages (from ultralytics==8.1.34) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /Users/kowsi/Library/Caches/pypoetry/virtualenvs/object-detect-dl-MR8rY4Ff-py3.12/lib/python3.12/site-packages (from ultralytics==8.1.34) (4.66.5)\n",
      "Requirement already satisfied: psutil in /Users/kowsi/Library/Caches/pypoetry/virtualenvs/object-detect-dl-MR8rY4Ff-py3.12/lib/python3.12/site-packages (from ultralytics==8.1.34) (6.0.0)\n",
      "Requirement already satisfied: py-cpuinfo in /Users/kowsi/Library/Caches/pypoetry/virtualenvs/object-detect-dl-MR8rY4Ff-py3.12/lib/python3.12/site-packages (from ultralytics==8.1.34) (9.0.0)\n",
      "Requirement already satisfied: thop>=0.1.1 in /Users/kowsi/Library/Caches/pypoetry/virtualenvs/object-detect-dl-MR8rY4Ff-py3.12/lib/python3.12/site-packages (from ultralytics==8.1.34) (0.1.1.post2209072238)\n",
      "Requirement already satisfied: pandas>=1.1.4 in /Users/kowsi/Library/Caches/pypoetry/virtualenvs/object-detect-dl-MR8rY4Ff-py3.12/lib/python3.12/site-packages (from ultralytics==8.1.34) (2.2.3)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /Users/kowsi/Library/Caches/pypoetry/virtualenvs/object-detect-dl-MR8rY4Ff-py3.12/lib/python3.12/site-packages (from ultralytics==8.1.34) (0.13.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/kowsi/Library/Caches/pypoetry/virtualenvs/object-detect-dl-MR8rY4Ff-py3.12/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics==8.1.34) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/kowsi/Library/Caches/pypoetry/virtualenvs/object-detect-dl-MR8rY4Ff-py3.12/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics==8.1.34) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/kowsi/Library/Caches/pypoetry/virtualenvs/object-detect-dl-MR8rY4Ff-py3.12/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics==8.1.34) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/kowsi/Library/Caches/pypoetry/virtualenvs/object-detect-dl-MR8rY4Ff-py3.12/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics==8.1.34) (1.4.7)\n",
      "Requirement already satisfied: numpy>=1.23 in /Users/kowsi/Library/Caches/pypoetry/virtualenvs/object-detect-dl-MR8rY4Ff-py3.12/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics==8.1.34) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/kowsi/Library/Caches/pypoetry/virtualenvs/object-detect-dl-MR8rY4Ff-py3.12/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics==8.1.34) (24.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/kowsi/Library/Caches/pypoetry/virtualenvs/object-detect-dl-MR8rY4Ff-py3.12/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics==8.1.34) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/kowsi/Library/Caches/pypoetry/virtualenvs/object-detect-dl-MR8rY4Ff-py3.12/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics==8.1.34) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/kowsi/Library/Caches/pypoetry/virtualenvs/object-detect-dl-MR8rY4Ff-py3.12/lib/python3.12/site-packages (from pandas>=1.1.4->ultralytics==8.1.34) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/kowsi/Library/Caches/pypoetry/virtualenvs/object-detect-dl-MR8rY4Ff-py3.12/lib/python3.12/site-packages (from pandas>=1.1.4->ultralytics==8.1.34) (2024.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/kowsi/Library/Caches/pypoetry/virtualenvs/object-detect-dl-MR8rY4Ff-py3.12/lib/python3.12/site-packages (from requests>=2.23.0->ultralytics==8.1.34) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/kowsi/Library/Caches/pypoetry/virtualenvs/object-detect-dl-MR8rY4Ff-py3.12/lib/python3.12/site-packages (from requests>=2.23.0->ultralytics==8.1.34) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kowsi/Library/Caches/pypoetry/virtualenvs/object-detect-dl-MR8rY4Ff-py3.12/lib/python3.12/site-packages (from requests>=2.23.0->ultralytics==8.1.34) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kowsi/Library/Caches/pypoetry/virtualenvs/object-detect-dl-MR8rY4Ff-py3.12/lib/python3.12/site-packages (from requests>=2.23.0->ultralytics==8.1.34) (2024.8.30)\n",
      "Requirement already satisfied: filelock in /Users/kowsi/Library/Caches/pypoetry/virtualenvs/object-detect-dl-MR8rY4Ff-py3.12/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics==8.1.34) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/kowsi/Library/Caches/pypoetry/virtualenvs/object-detect-dl-MR8rY4Ff-py3.12/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics==8.1.34) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/kowsi/Library/Caches/pypoetry/virtualenvs/object-detect-dl-MR8rY4Ff-py3.12/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics==8.1.34) (1.13.3)\n",
      "Requirement already satisfied: networkx in /Users/kowsi/Library/Caches/pypoetry/virtualenvs/object-detect-dl-MR8rY4Ff-py3.12/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics==8.1.34) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/kowsi/Library/Caches/pypoetry/virtualenvs/object-detect-dl-MR8rY4Ff-py3.12/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics==8.1.34) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/kowsi/Library/Caches/pypoetry/virtualenvs/object-detect-dl-MR8rY4Ff-py3.12/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics==8.1.34) (2024.9.0)\n",
      "Requirement already satisfied: setuptools in /Users/kowsi/Library/Caches/pypoetry/virtualenvs/object-detect-dl-MR8rY4Ff-py3.12/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics==8.1.34) (75.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/kowsi/Library/Caches/pypoetry/virtualenvs/object-detect-dl-MR8rY4Ff-py3.12/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics==8.1.34) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/kowsi/Library/Caches/pypoetry/virtualenvs/object-detect-dl-MR8rY4Ff-py3.12/lib/python3.12/site-packages (from jinja2->torch>=1.8.0->ultralytics==8.1.34) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/kowsi/Library/Caches/pypoetry/virtualenvs/object-detect-dl-MR8rY4Ff-py3.12/lib/python3.12/site-packages (from sympy->torch>=1.8.0->ultralytics==8.1.34) (1.3.0)\n",
      "Building wheels for collected packages: ultralytics\n",
      "  Building wheel for ultralytics (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ultralytics: filename=ultralytics-8.1.34-py3-none-any.whl size=731463 sha256=da96e1482c2c05b8dd432bc5b4a2b0722f1f54a5a53f9468f0d34b8f59d73c3d\n",
      "  Stored in directory: /private/var/folders/bc/5yqxxc8n0w9d8wlpd4j86kv00000gn/T/pip-ephem-wheel-cache-9nuvfyw2/wheels/a8/45/60/2207492f18f2b08afa3351d7d93bd7d7515366e26c3ed571ea\n",
      "Successfully built ultralytics\n",
      "Installing collected packages: ultralytics\n",
      "  Attempting uninstall: ultralytics\n",
      "    Found existing installation: ultralytics 8.3.3\n",
      "    Uninstalling ultralytics-8.3.3:\n",
      "      Successfully uninstalled ultralytics-8.3.3\n",
      "Successfully installed ultralytics-8.1.34\n"
     ]
    }
   ],
   "source": [
    "!pip install ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Download weights (e.g., `yolov10n.pt`) and place them in a `weights/` folder. Then update any `weights=`/`model=` arguments to point to your file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 8273,
     "status": "ok",
     "timestamp": 1727460247715,
     "user": {
      "displayName": "Bryen Param",
      "userId": "02690284411531247689"
     },
     "user_tz": -120
    },
    "id": "F8XSSyF7sbCr",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "588141aa-54a9-4e97-fc48-ed8a11901e7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded (file_name)\n",
      "Downloaded (file_name)\n",
      "Downloaded (file_name)\n",
      "Downloaded (file_name)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m urls:\n\u001b[1;32m     20\u001b[0m   file_name \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(weights_dir, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(url))\n\u001b[0;32m---> 21\u001b[0m   \u001b[43murllib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloaded (file_name)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.6/Frameworks/Python.framework/Versions/3.12/lib/python3.12/urllib/request.py:268\u001b[0m, in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reporthook:\n\u001b[1;32m    266\u001b[0m     reporthook(blocknum, bs, size)\n\u001b[0;32m--> 268\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m block \u001b[38;5;241m:=\u001b[39m \u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbs\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    269\u001b[0m     read \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(block)\n\u001b[1;32m    270\u001b[0m     tfp\u001b[38;5;241m.\u001b[39mwrite(block)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.6/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py:479\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    478\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[0;32m--> 479\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.6/Frameworks/Python.framework/Versions/3.12/lib/python3.12/socket.py:720\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 720\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    722\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.6/Frameworks/Python.framework/Versions/3.12/lib/python3.12/ssl.py:1251\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1247\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1248\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1249\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1250\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1252\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.6/Frameworks/Python.framework/Versions/3.12/lib/python3.12/ssl.py:1103\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1103\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1104\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1105\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib. request\n",
    "\n",
    "# Create a directory for the weights in the current working directory\n",
    "weights_dir = os.path.join(os.getcwd(), \"weights\")\n",
    "os. makedirs(weights_dir, exist_ok=True)\n",
    "\n",
    "# URLs of the weight files\n",
    "urls = [\n",
    "    \"https://github.com/jameslahm/yolov10/releases/download/v1.0/yolov10n.pt\",\n",
    "    \"https://github.com/jameslahm/yolov10/releases/download/v1.0/yolov10s.pt\",\n",
    "    \"https://github.com/jameslahm/yolov10/releases/download/v1.0/yolov10m.pt\",\n",
    "    \"https://github.com/jameslahm/yolov10/releases/download/v1.0/yolov10b.pt\",\n",
    "    \"https://github.com/jameslahm/yolov10/releases/download/v1.0/yolov10x.pt\",\n",
    "    \"https://github.com/jameslahm/yolov10/releases/download/v1.0/yolov10l.pt\"\n",
    "]\n",
    "\n",
    "# Download each file\n",
    "for url in urls:\n",
    "  file_name = os.path.join(weights_dir, os.path.basename(url))\n",
    "  urllib. request.urlretrieve(url, file_name)\n",
    "  print(f\"Downloaded {file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test prediction with base YOLOv10n Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 11094,
     "status": "ok",
     "timestamp": 1726834526283,
     "user": {
      "displayName": "Bryen Param",
      "userId": "02690284411531247689"
     },
     "user_tz": -120
    },
    "id": "f06PVLofvMMI",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "216b7c81-a189-4e29-f8a3-7ba849e208ad",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ultralytics/nn/tasks.py:733: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(file, map_location=\"cpu\")\n",
      "Ultralytics YOLOv8.1.34 🚀 Python-3.10.12 torch-2.4.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
      "YOLOv10n summary (fused): 285 layers, 2762608 parameters, 63840 gradients, 8.6 GFLOPs\n",
      "\n",
      "image 1/1 /content/yolov10/test_images/IMG_8455.jpg: 480x640 1 0, 1 63, 1 76, 41.3ms\n",
      "Speed: 14.2ms preprocess, 41.3ms inference, 396.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
      "💡 Learn more at https://docs.ultralytics.com/modes/predict\n"
     ]
    }
   ],
   "source": [
    "# Update the values below to point to YOUR files:\n",
    "#  - source=<path to your image/video/folder>\n",
    "#  - weights/model=<path to your .pt weights> (e.g., ../weights/yolov10n.pt)\n",
    "!yolo task=detect mode=predict conf=0.25 save=True model=../weights/yolov10n.pt source=test.mp4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5lmRv3NBpn1a"
   },
   "source": [
    "### Data Augmentation\n",
    "Here we apply data augmentation techniques (such as flips, brightness/contrast changes, or noise).  \n",
    "The goal is to increase dataset variety, improve robustness, and help the model generalize better  \n",
    "to different real-world conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 3209,
     "status": "ok",
     "timestamp": 1727453935189,
     "user": {
      "displayName": "Bryen Param",
      "userId": "02690284411531247689"
     },
     "user_tz": -120
    },
    "id": "s_OkivFtOCpj",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "ef9831e2-d562-4713-b5a8-4e7bafe6202d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: albumentations in /Users/kowsi/Library/Caches/pypoetry/virtualenvs/object-detect-dl-MR8rY4Ff-py3.12/lib/python3.12/site-packages (1.4.16)\n",
      "Requirement already satisfied: numpy>=1.24.4 in /Users/kowsi/Library/Caches/pypoetry/virtualenvs/object-detect-dl-MR8rY4Ff-py3.12/lib/python3.12/site-packages (from albumentations) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.10.0 in /Users/kowsi/Library/Caches/pypoetry/virtualenvs/object-detect-dl-MR8rY4Ff-py3.12/lib/python3.12/site-packages (from albumentations) (1.14.1)\n",
      "Requirement already satisfied: scikit-image>=0.21.0 in /Users/kowsi/Library/Caches/pypoetry/virtualenvs/object-detect-dl-MR8rY4Ff-py3.12/lib/python3.12/site-packages (from albumentations) (0.24.0)\n",
      "Requirement already satisfied: PyYAML in /Users/kowsi/Library/Caches/pypoetry/virtualenvs/object-detect-dl-MR8rY4Ff-py3.12/lib/python3.12/site-packages (from albumentations) (6.0.2)\n",
      "Requirement already satisfied: pydantic>=2.7.0 in /Users/kowsi/Library/Caches/pypoetry/virtualenvs/object-detect-dl-MR8rY4Ff-py3.12/lib/python3.12/site-packages (from albumentations) (2.9.2)\n",
      "Requirement already satisfied: albucore==0.0.17 in /Users/kowsi/Library/Caches/pypoetry/virtualenvs/object-detect-dl-MR8rY4Ff-py3.12/lib/python3.12/site-packages (from albumentations) (0.0.17)\n",
      "Requirement already satisfied: eval-type-backport in /Users/kowsi/Library/Caches/pypoetry/virtualenvs/object-detect-dl-MR8rY4Ff-py3.12/lib/python3.12/site-packages (from albumentations) (0.2.0)\n",
      "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /Users/kowsi/Library/Caches/pypoetry/virtualenvs/object-detect-dl-MR8rY4Ff-py3.12/lib/python3.12/site-packages (from albumentations) (4.10.0.84)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/kowsi/Library/Caches/pypoetry/virtualenvs/object-detect-dl-MR8rY4Ff-py3.12/lib/python3.12/site-packages (from pydantic>=2.7.0->albumentations) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /Users/kowsi/Library/Caches/pypoetry/virtualenvs/object-detect-dl-MR8rY4Ff-py3.12/lib/python3.12/site-packages (from pydantic>=2.7.0->albumentations) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/kowsi/Library/Caches/pypoetry/virtualenvs/object-detect-dl-MR8rY4Ff-py3.12/lib/python3.12/site-packages (from pydantic>=2.7.0->albumentations) (4.12.2)\n",
      "Requirement already satisfied: networkx>=2.8 in /Users/kowsi/Library/Caches/pypoetry/virtualenvs/object-detect-dl-MR8rY4Ff-py3.12/lib/python3.12/site-packages (from scikit-image>=0.21.0->albumentations) (3.3)\n",
      "Requirement already satisfied: pillow>=9.1 in /Users/kowsi/Library/Caches/pypoetry/virtualenvs/object-detect-dl-MR8rY4Ff-py3.12/lib/python3.12/site-packages (from scikit-image>=0.21.0->albumentations) (10.4.0)\n",
      "Requirement already satisfied: imageio>=2.33 in /Users/kowsi/Library/Caches/pypoetry/virtualenvs/object-detect-dl-MR8rY4Ff-py3.12/lib/python3.12/site-packages (from scikit-image>=0.21.0->albumentations) (2.35.1)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /Users/kowsi/Library/Caches/pypoetry/virtualenvs/object-detect-dl-MR8rY4Ff-py3.12/lib/python3.12/site-packages (from scikit-image>=0.21.0->albumentations) (2024.9.20)\n",
      "Requirement already satisfied: packaging>=21 in /Users/kowsi/Library/Caches/pypoetry/virtualenvs/object-detect-dl-MR8rY4Ff-py3.12/lib/python3.12/site-packages (from scikit-image>=0.21.0->albumentations) (24.1)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /Users/kowsi/Library/Caches/pypoetry/virtualenvs/object-detect-dl-MR8rY4Ff-py3.12/lib/python3.12/site-packages (from scikit-image>=0.21.0->albumentations) (0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2603,
     "status": "ok",
     "timestamp": 1727456752661,
     "user": {
      "displayName": "Bryen Param",
      "userId": "02690284411531247689"
     },
     "user_tz": -120
    },
    "id": "sRdZLvlWch_0",
    "outputId": "b2c887c1-cc04-4cf1-e118-734b57ead3d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /Users/kowsi/Library/Caches/pypoetry/virtualenvs/object-detect-dl-MR8rY4Ff-py3.12/lib/python3.12/site-packages (4.66.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install tqdm to display progress bars while processing images.\n",
    "pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starter cell for a **simple data augmentation** pipeline (resize, flips, color jitter, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First simple Data augmentation\n",
    "augmentations = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),  # Flip the image horizontally 50% of the time\n",
    "    A.VerticalFlip(p=0.5),    # Flip the image vertically 50% of the time\n",
    "    A.RandomBrightnessContrast(p=1),  # Always adjust brightness and contrast\n",
    "    A.MotionBlur(p=1),        # Always apply motion blur\n",
    "    A.HueSaturationValue(p=1),  # Always adjust hue, saturation, and value\n",
    "], bbox_params=A.BboxParams(format='yolo', label_fields=['category_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Template for a **richer augmentation pipeline** (e.g., geometric transforms, cutout, blur)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More Complex Data augmentation\n",
    "augmentations_2 = A.Compose([\n",
    "    # Rotation for object angles\n",
    "    A.Rotate(limit=45, p=1),\n",
    "    # Perspective changes\n",
    "    A.Affine(scale=(0.8, 1.2), translate_percent=(0.1, 0.1), rotate=0, shear=15, p=0.5),\n",
    "    # Light adjustments\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=1),\n",
    "    # Light effects\n",
    "    A.RandomShadow(shadow_roi=(0, 0.5, 1, 1), shadow_dimension=5, p=0.5),\n",
    "    A.RandomRain(slant_lower=-10, slant_upper=10, p=0.2),  # Light reflections\n",
    "    # Motion effects\n",
    "    A.MotionBlur(blur_limit=(3, 7), p=0.5), \n",
    "    # Image noise\n",
    "    A.GaussNoise(var_limit=(10.0, 50.0), p=0.5),\n",
    "    # Color changes\n",
    "    A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=1),\n",
    "    # Simulate blocked views\n",
    "    A.CoarseDropout(max_holes=8, max_height=16, max_width=16, min_holes=1, min_height=8, min_width=8, fill_value=0, p=0.5),\n",
    "], bbox_params=A.BboxParams(format='yolo', label_fields=['category_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "id": "r4W7NVZOenCT",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for InitSchema\n  Value error, If 'border_mode' is set to 'BORDER_CONSTANT', 'value' must be provided. [type=value_error, input_value={'min_height': 512, 'min_...e, 'always_apply': None}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/value_error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 43\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Define our augmentation pipeline\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# This will apply random transformations to our images\u001b[39;00m\n\u001b[1;32m      8\u001b[0m augmentations \u001b[38;5;241m=\u001b[39m A\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m      9\u001b[0m     A\u001b[38;5;241m.\u001b[39mHorizontalFlip(p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m),  \u001b[38;5;66;03m# Flip horizontally 50% of the time\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     A\u001b[38;5;241m.\u001b[39mVerticalFlip(p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m),    \u001b[38;5;66;03m# Flip vertically 30% of the time\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     A\u001b[38;5;241m.\u001b[39mRotate(limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.4\u001b[39m),\n\u001b[1;32m     29\u001b[0m ], bbox_params\u001b[38;5;241m=\u001b[39mA\u001b[38;5;241m.\u001b[39mBboxParams(\u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myolo\u001b[39m\u001b[38;5;124m'\u001b[39m, label_fields\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m     32\u001b[0m augmentations_4_not_tested \u001b[38;5;241m=\u001b[39m A\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m     33\u001b[0m     A\u001b[38;5;241m.\u001b[39mHorizontalFlip(p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m),  \u001b[38;5;66;03m# Flip horizontally 50% of the time\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     A\u001b[38;5;241m.\u001b[39mVerticalFlip(p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m),    \u001b[38;5;66;03m# Flip vertically 30% of the time\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     A\u001b[38;5;241m.\u001b[39mRandomBrightnessContrast(p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),  \u001b[38;5;66;03m# Always adjust brightness and contrast\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     A\u001b[38;5;241m.\u001b[39mMotionBlur(p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m),      \u001b[38;5;66;03m# Apply motion blur to simulate slight movement\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     A\u001b[38;5;241m.\u001b[39mHueSaturationValue(p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m),  \u001b[38;5;66;03m# Adjust hue and saturation to mimic lighting changes\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# Shift objects away from the center to simulate different locations\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     A\u001b[38;5;241m.\u001b[39mShiftScaleRotate(shift_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m, scale_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, rotate_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m),\n\u001b[1;32m     41\u001b[0m \n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# Add padding to ensure sharp objects are not always centered\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m     \u001b[43mA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPadIfNeeded\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmin_height\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_width\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mborder_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     44\u001b[0m \n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m# Slight rotation to cover orientation changes\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     A\u001b[38;5;241m.\u001b[39mRotate(limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.4\u001b[39m),\n\u001b[1;32m     47\u001b[0m ], bbox_params\u001b[38;5;241m=\u001b[39mA\u001b[38;5;241m.\u001b[39mBboxParams(\u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myolo\u001b[39m\u001b[38;5;124m'\u001b[39m, label_fields\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m     50\u001b[0m augmentations_3 \u001b[38;5;241m=\u001b[39m A\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m     51\u001b[0m     A\u001b[38;5;241m.\u001b[39mHorizontalFlip(p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.4\u001b[39m),  \u001b[38;5;66;03m# Reduced probability, still useful\u001b[39;00m\n\u001b[1;32m     52\u001b[0m     A\u001b[38;5;241m.\u001b[39mVerticalFlip(p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m),    \u001b[38;5;66;03m# Vertical flip happens less often to avoid unrealistic scenarios\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \n\u001b[1;32m     75\u001b[0m ], bbox_params\u001b[38;5;241m=\u001b[39mA\u001b[38;5;241m.\u001b[39mBboxParams(\u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myolo\u001b[39m\u001b[38;5;124m'\u001b[39m, label_fields\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m     78\u001b[0m augmentations_2 \u001b[38;5;241m=\u001b[39m A\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;66;03m# Flip the image to cover different angles\u001b[39;00m\n\u001b[1;32m     80\u001b[0m     A\u001b[38;5;241m.\u001b[39mHorizontalFlip(p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \n\u001b[1;32m    108\u001b[0m ], bbox_params\u001b[38;5;241m=\u001b[39mA\u001b[38;5;241m.\u001b[39mBboxParams(\u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myolo\u001b[39m\u001b[38;5;124m'\u001b[39m, label_fields\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/object-detect-dl-MR8rY4Ff-py3.12/lib/python3.12/site-packages/albumentations/core/validation.py:35\u001b[0m, in \u001b[0;36mValidatedTransformMeta.__new__.<locals>.custom_init\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m         full_kwargs[parameter_name] \u001b[38;5;241m=\u001b[39m parameter\u001b[38;5;241m.\u001b[39mdefault\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# No try-except block needed as we want the exception to propagate naturally\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[43mdct\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mInitSchema\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfull_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m validated_kwargs \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mmodel_dump()\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name_arg \u001b[38;5;129;01min\u001b[39;00m kwargs:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/object-detect-dl-MR8rY4Ff-py3.12/lib/python3.12/site-packages/pydantic/main.py:212\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[1;32m    211\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 212\u001b[0m validated_self \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[1;32m    214\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    215\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    216\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    218\u001b[0m         category\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    219\u001b[0m     )\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for InitSchema\n  Value error, If 'border_mode' is set to 'BORDER_CONSTANT', 'value' must be provided. [type=value_error, input_value={'min_height': 512, 'min_...e, 'always_apply': None}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/value_error"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_yolo_annotation(annotation_file):\n",
    "    \"\"\"Load YOLO format annotations from a file.\"\"\"\n",
    "    with open(annotation_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    # Extract bounding boxes and category IDs\n",
    "    bboxes = [list(map(float, line.strip().split()[1:])) for line in lines]\n",
    "    category_ids = [int(line.strip().split()[0]) for line in lines]\n",
    "    return bboxes, category_ids\n",
    "\n",
    "def save_augmented_data(image, bboxes, category_ids, output_img_path, output_label_path):\n",
    "    \"\"\"Save the augmented image and its corresponding YOLO labels.\"\"\"\n",
    "    cv2.imwrite(output_img_path, image)\n",
    "    with open(output_label_path, 'w') as f:\n",
    "        for bbox, class_id in zip(bboxes, category_ids):\n",
    "            f.write(f\"{class_id} {' '.join(map(str, bbox))}\\n\")\n",
    "\n",
    "def augment_data(image_dir, label_dir, output_image_dir, output_label_dir):\n",
    "    \"\"\"Main function to augment data.\"\"\"\n",
    "    # Create output directories if they don't exist\n",
    "    os.makedirs(output_image_dir, exist_ok=True)\n",
    "    os.makedirs(output_label_dir, exist_ok=True)\n",
    "\n",
    "    # Get all image files\n",
    "    image_files = [f for f in os.listdir(image_dir) if f.lower().endswith((\".jpeg\", \".jpg\"))]\n",
    "\n",
    "    # Process each image\n",
    "    for img_file in tqdm(image_files, desc=\"Augmenting Images\"):\n",
    "        img_path = os.path.join(image_dir, img_file)\n",
    "        label_path = os.path.join(label_dir, os.path.splitext(img_file)[0] + \".txt\")\n",
    "\n",
    "        # Read the image and its annotations\n",
    "        image = cv2.imread(img_path)\n",
    "        bboxes, category_ids = load_yolo_annotation(label_path)\n",
    "\n",
    "        # Save the original image and labels\n",
    "        save_augmented_data(image, bboxes, category_ids,\n",
    "                            os.path.join(output_image_dir, img_file),\n",
    "                            os.path.join(output_label_dir, os.path.splitext(img_file)[0] + \".txt\"))\n",
    "\n",
    "        # Generate 10 augmented versions of each image \n",
    "        for i in range(10): # Change the number of versions if needed\n",
    "            # Apply augmentations -> Select the appropriate augmentation\n",
    "            augmented = augmentations(image=image, bboxes=bboxes, category_ids=category_ids)\n",
    "\n",
    "            # Create unique filenames for augmented data\n",
    "            output_img_path = os.path.join(output_image_dir, f\"{os.path.splitext(img_file)[0]}_aug_{i+1}.jpg\")\n",
    "            output_label_path = os.path.join(output_label_dir, f\"{os.path.splitext(img_file)[0]}_aug_{i+1}.txt\")\n",
    "\n",
    "            # Save augmented image and updated labels\n",
    "            save_augmented_data(augmented['image'], augmented['bboxes'], augmented['category_ids'],\n",
    "                                output_img_path, output_label_path)\n",
    "\n",
    "    # Calculate and print total number of images generated\n",
    "    total_images = len(image_files) * 11  # Original + 10 augmented versions\n",
    "    print(f\"Total images generated: {total_images}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set these paths to match your file structure\n",
    "    image_dir = \"original_data/first_approach_multi_env_data/multi_env_original_images\"\n",
    "    label_dir = \"original_data/first_approach_multi_env_data/multi_env_original_labels\"\n",
    "    output_image_dir = \"final_augmented_base_images\"\n",
    "    output_label_dir = \"final_augmented_base_labels\"\n",
    "\n",
    "    # Run the augmentation process\n",
    "    augment_data(image_dir, label_dir, output_image_dir, output_label_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tTO9DRUhpeMG"
   },
   "source": [
    "## Rearange dataset structure for fine tuning\n",
    "In this step, we organize the dataset into the YOLO format (`images/train`, `images/val`, `labels/train`, `labels/val`).  \n",
    "This structure is required for training and ensures that YOLO can properly read both images and annotations  \n",
    "during the fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data pairs found: 252\n",
      "Dataset has been split and copied successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Set seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Define paths for images and labels\n",
    "images_path = 'final_augmented_base_images'\n",
    "labels_path = 'final_augmented_base_labels'\n",
    "\n",
    "# Destination directories for images and labels\n",
    "train_dir = 'datasets/images/train'\n",
    "val_dir = 'datasets/images/val'\n",
    "test_dir = 'datasets/images/test'\n",
    "train_label_dir = 'datasets/labels/train'\n",
    "val_label_dir = 'datasets/labels/val'\n",
    "test_label_dir = 'datasets/labels/test'\n",
    "\n",
    "# Create the directories if they don't exist\n",
    "Path(train_dir).mkdir(parents=True, exist_ok=True)\n",
    "Path(val_dir).mkdir(parents=True, exist_ok=True)\n",
    "Path(test_dir).mkdir(parents=True, exist_ok=True)\n",
    "Path(train_label_dir).mkdir(parents=True, exist_ok=True)\n",
    "Path(val_label_dir).mkdir(parents=True, exist_ok=True)\n",
    "Path(test_label_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# List all images (allowing for different cases in extensions)\n",
    "images = [f for f in os.listdir(images_path) if f.lower().endswith(('.jpg', '.jpeg'))]\n",
    "\n",
    "# List all labels\n",
    "labels = [f for f in os.listdir(labels_path) if f.endswith('.txt')]\n",
    "\n",
    "# Ensure each image has a corresponding label\n",
    "# We match by base name, ignoring file extension case (i.e., 'image1.jpg' with 'image1.txt')\n",
    "images = sorted(images)\n",
    "labels = sorted(labels)\n",
    "# Filter the data to only include pairs where both image and label exist\n",
    "data = []\n",
    "for image_file in images:\n",
    "    image_base = os.path.splitext(image_file)[0]\n",
    "    label_file = f\"{image_base}.txt\"\n",
    "    if label_file in labels:\n",
    "        data.append((image_file, label_file))\n",
    "# Check that we have valid image-label pairs\n",
    "assert len(data) > 0, \"No matching image-label pairs found!\"\n",
    "print(f\"Total data pairs found: {len(data)}\")\n",
    "\n",
    "# Shuffle the data\n",
    "random.shuffle(data)\n",
    "\n",
    "# Split the dataset (80% train, 10% val, 10% test)\n",
    "train_split = int(0.8 * len(data))\n",
    "val_split = int(0.9 * len(data))\n",
    "train_data = data[:train_split]\n",
    "val_data = data[train_split:val_split]\n",
    "test_data = data[val_split:]\n",
    "\n",
    "# Function to copy files\n",
    "def copy_files(data, image_dest, label_dest):\n",
    "    for image_file, label_file in data:\n",
    "        shutil.copy(os.path.join(images_path, image_file), os.path.join(image_dest, image_file))\n",
    "        shutil.copy(os.path.join(labels_path, label_file), os.path.join(label_dest, label_file))\n",
    "        \n",
    "# Copy train, val, and test files\n",
    "copy_files(train_data, train_dir, train_label_dir)\n",
    "copy_files(val_data, val_dir, val_label_dir)\n",
    "copy_files(test_data, test_dir, test_label_dir)\n",
    "print(\"Dataset has been split and copied successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook prepared the dataset for fine-tuning our YOLOv10 model.  \n",
    "We organized the data into the correct YOLO format, applied augmentations to improve robustness,  \n",
    "and verified that annotations and images are aligned.  \n",
    "\n",
    "With the dataset now ready, the next step is to use it in the training pipeline (`finetuning_yolo.ipynb`)  \n",
    "to fine-tune the model and evaluate its performance on our custom task."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyORI5J9mDOP7OMWhYRWPPRi",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
